LLM Tool Guardrails is a lightweight middleware system for detecting and preventing hallucinated tool use in large language models (LLMs). It works by intercepting and validating function/tool calls made by the model, then filtering, rewriting, or blocking hallucinated or invalid ones.

This project aims to improve the reliability and safety of tool-augmented LLMs by introducing programmatic checks before external tool execution. It supports rule-based and prompt-aware strategies and is compatible with OpenAI function calling, LangChain tools, and similar LLM frameworks.
